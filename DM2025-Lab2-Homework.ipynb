{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: é™³ä¿Šä»»\n",
    "\n",
    "Student ID: 114062591\n",
    "\n",
    "GitHub ID: ArnoldChen0601\n",
    "\n",
    "Kaggle name: Jun-Ren Chen\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "This project predicts 6 emotion categories (anger, disgust, fear, sadness, surprise, joy) from Twitter posts. Detailed analysis was performed in [Phase3_EDA.ipynb](./Phase3_EDA.ipynb).\n",
    "\n",
    "**Data Loading:**\n",
    "- [final_posts.json](./Phase3_Dataset/final_posts.json): 64,171 posts\n",
    "- [emotion.csv](./Phase3_Dataset/emotion.csv): 47,890 training labels\n",
    "- [data_identification.csv](./Phase3_Dataset/data_identification.csv): train/test split (16,281 test samples)\n",
    "\n",
    "**Key EDA Findings (from [Phase3_EDA.ipynb](./Phase3_EDA.ipynb)):**\n",
    "- **Class Imbalance:** 20.1:1 ratio (joy: 49.7% vs disgust: 2.5%)\n",
    "- **Text Length:** Anger texts longest (85.9 chars), Joy shortest (70.7 chars)\n",
    "- **Punctuation Patterns:** Joy uses !, Surprise uses ?, Fear uses ...\n",
    "\n",
    "**Text Preprocessing:**\n",
    "- **Deep Learning (DistilRoBERTa):** Keep original text without cleaning; emojis are preserved as they strongly indicate emotions (e.g., ðŸ˜Š for joy, ðŸ˜¢ for sadness)\n",
    "- **Traditional ML:** Lowercase + NLTK tokenization; `max_length=128` covers 95% of texts\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "**1. TF-IDF Features:** N-gram (1,2), max 5000 features, `sublinear_tf=True`\n",
    "\n",
    "**2. Word2Vec Features:** Google News 300-dim vectors with average pooling\n",
    "\n",
    "**3. K-Means Clustering:** K=20 clusters on Word2Vec, one-hot encoded\n",
    "\n",
    "**4. Statistical Features (from [Phase3_EDA.ipynb](./Phase3_EDA.ipynb)):**\n",
    "- Text length, word count, punctuation counts (!, ?, ...)\n",
    "- Caps ratio, hashtag count, sentiment word counts\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "**Stacking Ensemble Architecture:**\n",
    "\n",
    "**Layer 1 - Base Models:**\n",
    "\n",
    "| Model | Representation | Classifier | Imbalance Strategy |\n",
    "|-------|----------------|------------|-------------------|\n",
    "| 1 | DistilRoBERTa | Transformer | Focal Loss (Î³=2.0) |\n",
    "| 2 | Word2Vec + K-Means | LinearSVC | class_weight='balanced' |\n",
    "| 3 | TF-IDF (1-2 gram) | Naive Bayes | fit_prior=True |\n",
    "\n",
    "**Layer 2 - Meta-Learner:** Logistic Regression on stacked probabilities (18-dim: 6 classes Ã— 3 models)\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "\n",
    "| Parameter | Value | Parameter | Value |\n",
    "|-----------|-------|-----------|-------|\n",
    "| batch_size | 16 | svm_c | 1.0 |\n",
    "| learning_rate | 2e-5 | svm_kernel | linear |\n",
    "| epochs | 10 | nb_alpha | 1.0 |\n",
    "| early_stopping | 5 | validation_size | 0.15 |\n",
    "\n",
    "**Focal Loss Î± (class weights):** disgust=8.0, fear=5.0, sadness=2.5, surprise=1.5, anger=0.8, joy=0.5\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Different Things Tried\n",
    "\n",
    "| Category | Tried | Selected | Reason |\n",
    "|----------|-------|----------|--------|\n",
    "| Model | BERT, RoBERTa | DistilRoBERTa | Pre-trained for emotion |\n",
    "| Loss | Cross Entropy | Focal Loss | Better for minority classes |\n",
    "| SVM Kernel | RBF | Linear | RBF too slow |\n",
    "| N-gram | (1,3) | (1,2) | Less sparse features |\n",
    "| Ensemble | Voting, 5-fold CV | Stacking | Best performance |\n",
    "\n",
    "**Hyperparameter Changes:** batch_size 32â†’16, epochs 3â†’10, svm_c 10â†’1, nb_alpha 0.1â†’1.0\n",
    "\n",
    "### 2.2 Insights Gained\n",
    "\n",
    "1. **Class Imbalance:** 20:1 ratio requires Focal Loss + balanced weights\n",
    "2. **Model Diversity:** Three different representations (Transformer, Word2Vec, TF-IDF) capture complementary patterns\n",
    "3. **EDA Value:** Punctuation patterns (!, ?, ...) correlate with emotions; text length varies by emotion\n",
    "4. **Emoji Importance:** Preserved emojis as strong emotion indicators for deep learning models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\NTHU_Projects\\DM2025-Lab2-Exercise\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Libraries imported successfully\n",
      "GPU Available: True\n",
      "   GPU Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1.1 Import Libraries ====================\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Scikit-learn: preprocessing, models, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# NLP processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word2Vec for word embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# PyTorch and Transformers for deep learning\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1.2 Configuration ====================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for all hyperparameters and paths\"\"\"\n",
    "    \n",
    "    # Data paths (all files in Phase3_Dataset folder)\n",
    "    DATA_DIR = Path(\"Phase3_Dataset\")\n",
    "    POSTS_FILE = DATA_DIR / \"final_posts.json\"\n",
    "    EMOTION_FILE = DATA_DIR / \"emotion.csv\"\n",
    "    IDENTIFICATION_FILE = DATA_DIR / \"data_identification.csv\"\n",
    "    \n",
    "    # DistilRoBERTa settings\n",
    "    DISTILROBERTA_MODEL = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 16                    # Reduced for memory stability\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EPOCHS = 10                        # Extended training for convergence\n",
    "    WARMUP_STEPS = 500\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2    # Compensates for smaller batch size\n",
    "    \n",
    "    # Training\n",
    "    VALIDATION_SIZE = 0.15             # 15% stratified validation split\n",
    "    EARLY_STOPPING_PATIENCE = 5        # More patience for improvement\n",
    "    \n",
    "    # Traditional ML settings\n",
    "    TFIDF_MAX_FEATURES = 5000\n",
    "    TFIDF_NGRAM = (1, 2)               # Reduced from (1,3) to avoid sparse features\n",
    "    KMEANS_CLUSTERS = 20               # Reduced from 30 to prevent overfitting\n",
    "    SVM_C = 1.0                        # Reduced from 10 to prevent overfitting\n",
    "    SVM_KERNEL = \"linear\"              # Faster than RBF for large datasets\n",
    "    SVM_MAX_ITER = 5000                # Increased for full convergence\n",
    "    NB_ALPHA = 1.0                     # Increased smoothing to reduce overfitting\n",
    "    \n",
    "    # Emotion labels (6 classes)\n",
    "    EMOTIONS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "    \n",
    "    # Focal Loss alpha weights (based on class distribution from EDA)\n",
    "    FOCAL_ALPHA = {\n",
    "        'disgust': 8.0,   # 2.5% - rarest, highest weight\n",
    "        'fear': 5.0,      # 4.2%\n",
    "        'sadness': 2.5,   # 8.2%\n",
    "        'surprise': 1.5,  # 13.1%\n",
    "        'anger': 0.8,     # 22.3%\n",
    "        'joy': 0.5        # 49.7% - most common, lowest weight\n",
    "    }\n",
    "    FOCAL_GAMMA = 2.0  # Focusing parameter\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 47890 training samples, 16281 test samples\n",
      "\n",
      "Class Distribution:\n",
      "emotion\n",
      "joy         23797\n",
      "anger       10694\n",
      "surprise     6281\n",
      "sadness      3926\n",
      "fear         2009\n",
      "disgust      1183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1.3 Data Loading ====================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and merge all data files from Phase3_Dataset folder into train/test DataFrames\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load posts from Phase3_Dataset/final_posts.json (nested JSON structure)\n",
    "    with open(Config.POSTS_FILE, 'r', encoding='utf-8') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    # Extract post_id and text from JSON\n",
    "    posts_list = []\n",
    "    for item in posts_data:\n",
    "        post_info = item['root']['_source']['post']\n",
    "        posts_list.append({\n",
    "            'id': post_info['post_id'],\n",
    "            'text': post_info['text'],\n",
    "            'hashtags': post_info.get('hashtags', [])\n",
    "        })\n",
    "    posts_df = pd.DataFrame(posts_list)\n",
    "    \n",
    "    # Load emotion labels from Phase3_Dataset/emotion.csv\n",
    "    emotion_df = pd.read_csv(Config.EMOTION_FILE)\n",
    "    \n",
    "    # Load train/test split info from Phase3_Dataset/data_identification.csv\n",
    "    identification_df = pd.read_csv(Config.IDENTIFICATION_FILE)\n",
    "    \n",
    "    # Merge all data\n",
    "    df = posts_df.merge(identification_df, on='id', how='left')\n",
    "    df = df.merge(emotion_df, on='id', how='left')\n",
    "    \n",
    "    # Split by 'split' column\n",
    "    train_df = df[df['split'] == 'train'].copy()\n",
    "    test_df = df[df['split'] == 'test'].copy()\n",
    "    \n",
    "    print(f\"Loaded {len(train_df)} training samples, {len(test_df)} test samples\")\n",
    "    return train_df, test_df\n",
    "\n",
    "# Load data\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "# Display class distribution (note: highly imbalanced, see Phase3_EDA.ipynb for details)\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(train_df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 2.1 Statistical Features Extraction ====================\n",
    "\n",
    "def extract_statistical_features(text):\n",
    "    \"\"\"Extract hand-crafted features based on EDA findings\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Length features\n",
    "    features['text_length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    \n",
    "    # Punctuation features (emotion-correlated from EDA)\n",
    "    features['exclamation_count'] = text.count('!')   # Joy uses most !\n",
    "    features['question_count'] = text.count('?')      # Surprise uses most ?\n",
    "    features['ellipsis_count'] = text.count('...')    # Fear uses most ...\n",
    "    \n",
    "    # Capitalization ratio\n",
    "    features['caps_ratio'] = sum(1 for c in text if c.isupper()) / (len(text) + 1)\n",
    "    \n",
    "    # Social media feature\n",
    "    features['hashtag_count'] = text.count('#')\n",
    "    \n",
    "    # Simple lexicon-based sentiment\n",
    "    positive_words = ['good', 'great', 'happy', 'love', 'wonderful', 'best', 'awesome', 'amazing']\n",
    "    negative_words = ['bad', 'hate', 'terrible', 'worst', 'awful', 'horrible', 'sad', 'angry']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    features['positive_word_count'] = sum(text_lower.count(w) for w in positive_words)\n",
    "    features['negative_word_count'] = sum(text_lower.count(w) for w in negative_words)\n",
    "    features['sentiment_diff'] = features['positive_word_count'] - features['negative_word_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic preprocessing for traditional NLP models\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec feature extractor defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 2.2 Word2Vec Feature Extractor ====================\n",
    "\n",
    "class Word2VecFeatureExtractor:\n",
    "    \"\"\"Convert text to 300-dim vectors using pre-trained Word2Vec\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"   Loading Word2Vec model...\", end='', flush=True)\n",
    "        # Load Google News 300-dim pre-trained vectors\n",
    "        self.model = api.load(\"word2vec-google-news-300\")\n",
    "        print(\" Done\")\n",
    "        \n",
    "    def text_to_vector(self, text):\n",
    "        \"\"\"Average pooling of word vectors\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vectors.append(self.model[token])\n",
    "            except KeyError:\n",
    "                pass  # Skip out-of-vocabulary words\n",
    "        \n",
    "        # Return average vector or zero vector if no words found\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Transform list of texts to matrix of vectors\"\"\"\n",
    "        vectors = [self.text_to_vector(t) for t in tqdm(texts, desc=\"   Word2Vec\", ncols=80, leave=False)]\n",
    "        return np.array(vectors)\n",
    "\n",
    "print(\"Word2Vec feature extractor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning components defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3.1 Deep Learning Components ====================\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DistilRoBERTa fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize text with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"Focal Loss: down-weights easy examples, focuses on hard ones\n",
    "    \n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    - alpha: class weights (higher for rare classes)\n",
    "    - gamma: focusing parameter (higher = more focus on hard examples)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # Probability of correct class\n",
    "        alpha = self.alpha.to(inputs.device)\n",
    "        alpha_t = alpha[targets]\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \"\"\"Custom HuggingFace Trainer using Focal Loss instead of CE\"\"\"\n",
    "    \n",
    "    def __init__(self, focal_alpha, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=Config.FOCAL_GAMMA)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = self.focal_loss(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "class SimpleProgressCallback(TrainerCallback):\n",
    "    \"\"\"Minimal progress display during training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_epochs = 0\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.total_epochs = int(args.num_train_epochs)\n",
    "        print(f\"\\nStarting training: {self.total_epochs} epochs\")\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        epoch = int(state.epoch) if state.epoch else 0\n",
    "        info = f\"Epoch {epoch}/{self.total_epochs}\"\n",
    "        if 'loss' in logs:\n",
    "            info += f\" | loss: {logs['loss']:.4f}\"\n",
    "        if 'eval_loss' in logs:\n",
    "            info += f\" | eval_loss: {logs['eval_loss']:.4f}\"\n",
    "        print(f\"\\r{info}\", end='', flush=True)\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print()\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(\"\\nTraining completed\\n\")\n",
    "\n",
    "print(\"Deep learning components defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilRoBERTa training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3.2 DistilRoBERTa Training Functions ====================\n",
    "\n",
    "def train_distilroberta(train_texts, train_labels, val_texts=None, val_labels=None):\n",
    "    \"\"\"Fine-tune DistilRoBERTa with Focal Loss for emotion classification\"\"\"\n",
    "    print(\"\\nTraining DistilRoBERTa...\")\n",
    "    \n",
    "    # Create validation split if not provided (15% stratified)\n",
    "    if val_texts is None:\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_texts, train_labels, \n",
    "            test_size=Config.VALIDATION_SIZE,\n",
    "            random_state=SEED,\n",
    "            stratify=train_labels\n",
    "        )\n",
    "    \n",
    "    # Load pre-trained model and tokenizer\n",
    "    print(\"   Loading model...\", end='', flush=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.DISTILROBERTA_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        Config.DISTILROBERTA_MODEL,\n",
    "        num_labels=len(Config.EMOTIONS),\n",
    "        ignore_mismatched_sizes=True  # Re-initialize classification head\n",
    "    )\n",
    "    print(\" Done\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, Config.MAX_LENGTH)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, Config.MAX_LENGTH)\n",
    "    \n",
    "    # Compute focal loss alpha weights in label order\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(Config.EMOTIONS)\n",
    "    focal_alpha = [Config.FOCAL_ALPHA[e] for e in label_encoder.classes_]\n",
    "    \n",
    "    # Training configuration (matching config.toml)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_distilroberta',\n",
    "        num_train_epochs=Config.EPOCHS,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=Config.BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY,\n",
    "        warmup_steps=Config.WARMUP_STEPS,\n",
    "        logging_steps=50,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        fp16=torch.cuda.is_available(),  # Mixed precision on GPU\n",
    "        seed=SEED,\n",
    "        disable_tqdm=True,\n",
    "        report_to='none',\n",
    "        log_level='error',\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    trainer = FocalLossTrainer(\n",
    "        focal_alpha=focal_alpha,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=Config.EARLY_STOPPING_PATIENCE),\n",
    "            SimpleProgressCallback()\n",
    "        ],\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_distilroberta(model, tokenizer, texts):\n",
    "    \"\"\"Get class probabilities from DistilRoBERTa\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    dataset = EmotionDataset(texts, None, tokenizer, Config.MAX_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"   Predicting\", ncols=80, leave=False):\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device)\n",
    "            )\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "print(\"DistilRoBERTa training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional ML training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3.3 Traditional ML Model Training ====================\n",
    "\n",
    "def train_word2vec_svm(train_texts, train_labels):\n",
    "    \"\"\"Train Word2Vec + Linear SVM pipeline\"\"\"\n",
    "    print(\"\\nTraining Word2Vec + SVM...\")\n",
    "    \n",
    "    # Extract Word2Vec features\n",
    "    w2v_extractor = Word2VecFeatureExtractor()\n",
    "    processed_texts = [preprocess_text(t) for t in tqdm(train_texts, desc=\"   Preprocessing\", ncols=80, leave=False)]\n",
    "    X_w2v = w2v_extractor.transform(processed_texts)\n",
    "    \n",
    "    # Add K-Means cluster features for semantic grouping (K=20 from config.toml)\n",
    "    kmeans = KMeans(n_clusters=Config.KMEANS_CLUSTERS, random_state=SEED, n_init=10, verbose=0)\n",
    "    cluster_ids = kmeans.fit_predict(X_w2v)\n",
    "    cluster_features = np.zeros((len(cluster_ids), Config.KMEANS_CLUSTERS))\n",
    "    cluster_features[np.arange(len(cluster_ids)), cluster_ids] = 1  # One-hot encoding\n",
    "    \n",
    "    # Combine Word2Vec + cluster features\n",
    "    X_combined = np.hstack([X_w2v, cluster_features])\n",
    "    \n",
    "    # Train LinearSVC with probability calibration (config.toml settings)\n",
    "    base_svm = LinearSVC(\n",
    "        C=Config.SVM_C,                   # 1.0 from config.toml\n",
    "        class_weight='balanced',          # Handle class imbalance\n",
    "        random_state=SEED,\n",
    "        max_iter=Config.SVM_MAX_ITER,     # 5000 for full convergence\n",
    "        dual='auto'\n",
    "    )\n",
    "    svm = CalibratedClassifierCV(base_svm, cv=3, method='sigmoid')\n",
    "    print(\"   Training SVM...\", end='', flush=True)\n",
    "    svm.fit(X_combined, train_labels)\n",
    "    print(\" Done\")\n",
    "    \n",
    "    return w2v_extractor, kmeans, svm\n",
    "\n",
    "\n",
    "def predict_word2vec_svm(w2v_extractor, kmeans, svm, texts):\n",
    "    \"\"\"Get class probabilities from Word2Vec + SVM\"\"\"\n",
    "    processed = [preprocess_text(t) for t in tqdm(texts, desc=\"   Preprocessing\", ncols=80, leave=False)]\n",
    "    X_w2v = w2v_extractor.transform(processed)\n",
    "    \n",
    "    # Apply same clustering transform\n",
    "    cluster_ids = kmeans.predict(X_w2v)\n",
    "    cluster_features = np.zeros((len(cluster_ids), Config.KMEANS_CLUSTERS))\n",
    "    cluster_features[np.arange(len(cluster_ids)), cluster_ids] = 1\n",
    "    \n",
    "    X_combined = np.hstack([X_w2v, cluster_features])\n",
    "    return svm.predict_proba(X_combined)\n",
    "\n",
    "\n",
    "def train_tfidf_nb(train_texts, train_labels):\n",
    "    \"\"\"Train TF-IDF + Multinomial Naive Bayes pipeline\"\"\"\n",
    "    print(\"\\nTraining TF-IDF + Naive Bayes...\")\n",
    "    \n",
    "    # TF-IDF vectorization with n-grams (1,2 from config.toml)\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=Config.TFIDF_MAX_FEATURES,\n",
    "        ngram_range=Config.TFIDF_NGRAM,   # (1, 2) from config.toml\n",
    "        sublinear_tf=True,                # Use log(tf) for smoothing\n",
    "        min_df=2, max_df=0.8,\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True\n",
    "    )\n",
    "    print(\"   Vectorizing...\", end='', flush=True)\n",
    "    X_tfidf = tfidf.fit_transform(train_texts)\n",
    "    print(\" Done\")\n",
    "    \n",
    "    # Extract statistical features (for potential future use)\n",
    "    stat_features = []\n",
    "    for text in tqdm(train_texts, desc=\"   Stats features\", ncols=80, leave=False):\n",
    "        stat_features.append(list(extract_statistical_features(text).values()))\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(np.array(stat_features))\n",
    "    \n",
    "    # Train Naive Bayes (alpha=1.0 from config.toml for more smoothing)\n",
    "    nb = MultinomialNB(alpha=Config.NB_ALPHA, fit_prior=True)\n",
    "    print(\"   Training NB...\", end='', flush=True)\n",
    "    nb.fit(X_tfidf, train_labels)\n",
    "    print(\" Done\")\n",
    "    \n",
    "    return tfidf, scaler, nb\n",
    "\n",
    "\n",
    "def predict_tfidf_nb(tfidf, scaler, nb, texts):\n",
    "    \"\"\"Get class probabilities from TF-IDF + Naive Bayes\"\"\"\n",
    "    X_tfidf = tfidf.transform(texts)\n",
    "    return nb.predict_proba(X_tfidf)\n",
    "\n",
    "print(\"Traditional ML training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking ensemble functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3.4 Stacking Ensemble ====================\n",
    "\n",
    "def train_stacking_ensemble(train_texts, train_labels):\n",
    "    \"\"\"\n",
    "    Train two-layer stacking ensemble:\n",
    "    Layer 1: DistilRoBERTa, Word2Vec+SVM, TF-IDF+NB (base models)\n",
    "    Layer 2: Logistic Regression (meta-learner)\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Stacking Ensemble...\\n\")\n",
    "    \n",
    "    # Encode labels to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(train_labels)\n",
    "    \n",
    "    print(f\"Training on {len(train_texts)} samples\\n\")\n",
    "    training_start = time.time()\n",
    "    \n",
    "    # ========== Layer 1: Train Base Models ==========\n",
    "    \n",
    "    # Model 1: DistilRoBERTa (deep learning, captures context)\n",
    "    print(\"=\"*50)\n",
    "    print(\"Training Model 1/3: DistilRoBERTa\")\n",
    "    print(\"=\"*50)\n",
    "    model_dr, tokenizer_dr = train_distilroberta(train_texts, encoded_labels)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Free GPU memory\n",
    "    \n",
    "    # Model 2: Word2Vec + SVM (captures word semantics)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Model 2/3: Word2Vec + SVM\")\n",
    "    print(\"=\"*50)\n",
    "    w2v_ext, kmeans, svm = train_word2vec_svm(train_texts, encoded_labels)\n",
    "    \n",
    "    # Model 3: TF-IDF + Naive Bayes (captures word frequencies)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Model 3/3: TF-IDF + Naive Bayes\")\n",
    "    print(\"=\"*50)\n",
    "    tfidf, scaler, nb = train_tfidf_nb(train_texts, encoded_labels)\n",
    "    \n",
    "    models = {\n",
    "        'distilroberta': (model_dr, tokenizer_dr),\n",
    "        'word2vec_svm': (w2v_ext, kmeans, svm),\n",
    "        'tfidf_nb': (tfidf, scaler, nb)\n",
    "    }\n",
    "    \n",
    "    # ========== Layer 2: Train Meta-Learner ==========\n",
    "    print(\"\\nTraining Meta-Learner...\")\n",
    "    \n",
    "    # Get base model predictions on training data\n",
    "    train_probs_dr = predict_distilroberta(model_dr, tokenizer_dr, train_texts)\n",
    "    train_probs_svm = predict_word2vec_svm(w2v_ext, kmeans, svm, train_texts)\n",
    "    train_probs_nb = predict_tfidf_nb(tfidf, scaler, nb, train_texts)\n",
    "    \n",
    "    # Stack predictions: 18 features (6 classes Ã— 3 models)\n",
    "    train_meta_features = np.hstack([train_probs_dr, train_probs_svm, train_probs_nb])\n",
    "    \n",
    "    # Train Logistic Regression to learn optimal model combination\n",
    "    meta_learner = LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED\n",
    "    )\n",
    "    meta_learner.fit(train_meta_features, encoded_labels)\n",
    "    \n",
    "    print(f\"\\nTraining completed in {(time.time() - training_start)/60:.1f} minutes\")\n",
    "    \n",
    "    return models, meta_learner, label_encoder\n",
    "\n",
    "\n",
    "def predict_stacking_ensemble(models, meta_learner, test_texts):\n",
    "    \"\"\"Generate predictions using the stacking ensemble\"\"\"\n",
    "    print(\"\\nPredicting on Test Set...\")\n",
    "    \n",
    "    # Unpack models\n",
    "    model_dr, tokenizer_dr = models['distilroberta']\n",
    "    w2v_ext, kmeans, svm = models['word2vec_svm']\n",
    "    tfidf, scaler, nb = models['tfidf_nb']\n",
    "    \n",
    "    # Get base model predictions\n",
    "    print(\"   Generating predictions from base models...\")\n",
    "    probs_dr = predict_distilroberta(model_dr, tokenizer_dr, test_texts)\n",
    "    probs_svm = predict_word2vec_svm(w2v_ext, kmeans, svm, test_texts)\n",
    "    probs_nb = predict_tfidf_nb(tfidf, scaler, nb, test_texts)\n",
    "    \n",
    "    # Stack and apply meta-learner\n",
    "    meta_features = np.hstack([probs_dr, probs_svm, probs_nb])\n",
    "    print(\"   Applying meta-learner...\")\n",
    "    \n",
    "    return meta_learner.predict(meta_features)\n",
    "\n",
    "print(\"Stacking ensemble functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 47890\n",
      "Test samples: 16281\n",
      "\n",
      "Training Stacking Ensemble...\n",
      "\n",
      "Training on 47890 samples\n",
      "\n",
      "==================================================\n",
      "Training Model 1/3: DistilRoBERTa\n",
      "==================================================\n",
      "\n",
      "Training DistilRoBERTa...\n",
      "   Loading model..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done\n",
      "\n",
      "Starting training: 10 epochs\n",
      "Epoch 0/10 | loss: 1.5919{'loss': 1.5919, 'grad_norm': 6.509062767028809, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.03929273084479371}\n",
      "Epoch 0/10 | loss: 1.4445{'loss': 1.4445, 'grad_norm': 7.025918006896973, 'learning_rate': 3.920000000000001e-06, 'epoch': 0.07858546168958742}\n",
      "Epoch 0/10 | loss: 1.4616{'loss': 1.4616, 'grad_norm': 7.594344615936279, 'learning_rate': 5.92e-06, 'epoch': 0.11787819253438114}\n",
      "Epoch 0/10 | loss: 1.1618{'loss': 1.1618, 'grad_norm': 8.269306182861328, 'learning_rate': 7.92e-06, 'epoch': 0.15717092337917485}\n",
      "Epoch 0/10 | loss: 1.1168{'loss': 1.1168, 'grad_norm': 11.21353530883789, 'learning_rate': 9.920000000000002e-06, 'epoch': 0.19646365422396855}\n",
      "Epoch 0/10 | loss: 1.0540{'loss': 1.054, 'grad_norm': 16.634931564331055, 'learning_rate': 1.188e-05, 'epoch': 0.2357563850687623}\n",
      "Epoch 0/10 | loss: 0.9238{'loss': 0.9238, 'grad_norm': 9.554232597351074, 'learning_rate': 1.3880000000000001e-05, 'epoch': 0.275049115913556}\n",
      "Epoch 0/10 | loss: 1.0975{'loss': 1.0975, 'grad_norm': 9.061256408691406, 'learning_rate': 1.588e-05, 'epoch': 0.3143418467583497}\n",
      "Epoch 0/10 | loss: 0.8777{'loss': 0.8777, 'grad_norm': 12.672636985778809, 'learning_rate': 1.788e-05, 'epoch': 0.35363457760314343}\n",
      "Epoch 0/10 | loss: 0.9953{'loss': 0.9953, 'grad_norm': 16.086870193481445, 'learning_rate': 1.9880000000000003e-05, 'epoch': 0.3929273084479371}\n",
      "Epoch 0/10 | loss: 0.9560{'loss': 0.956, 'grad_norm': 14.711865425109863, 'learning_rate': 1.9923139820114473e-05, 'epoch': 0.43222003929273084}\n",
      "Epoch 0/10 | loss: 0.9148{'loss': 0.9148, 'grad_norm': 15.829144477844238, 'learning_rate': 1.9841373671300084e-05, 'epoch': 0.4715127701375246}\n",
      "Epoch 0/10 | loss: 0.9663{'loss': 0.9663, 'grad_norm': 10.667122840881348, 'learning_rate': 1.9759607522485693e-05, 'epoch': 0.5108055009823183}\n",
      "Epoch 0/10 | loss: 0.9111{'loss': 0.9111, 'grad_norm': 5.2184529304504395, 'learning_rate': 1.9677841373671304e-05, 'epoch': 0.550098231827112}\n",
      "Epoch 0/10 | loss: 0.9538{'loss': 0.9538, 'grad_norm': 9.629125595092773, 'learning_rate': 1.95977105478332e-05, 'epoch': 0.5893909626719057}\n",
      "Epoch 0/10 | loss: 0.8938{'loss': 0.8938, 'grad_norm': 5.938371181488037, 'learning_rate': 1.9515944399018808e-05, 'epoch': 0.6286836935166994}\n",
      "Epoch 0/10 | loss: 0.8871{'loss': 0.8871, 'grad_norm': 18.106176376342773, 'learning_rate': 1.9434178250204416e-05, 'epoch': 0.6679764243614931}\n",
      "Epoch 0/10 | loss: 0.8918{'loss': 0.8918, 'grad_norm': 13.952373504638672, 'learning_rate': 1.9352412101390024e-05, 'epoch': 0.7072691552062869}\n",
      "Epoch 0/10 | loss: 0.9725{'loss': 0.9725, 'grad_norm': 11.712153434753418, 'learning_rate': 1.9270645952575636e-05, 'epoch': 0.7465618860510805}\n",
      "Epoch 0/10 | loss: 0.8211{'loss': 0.8211, 'grad_norm': 15.95712947845459, 'learning_rate': 1.9188879803761244e-05, 'epoch': 0.7858546168958742}\n",
      "Epoch 0/10 | loss: 1.0252{'loss': 1.0252, 'grad_norm': 10.562555313110352, 'learning_rate': 1.9107113654946855e-05, 'epoch': 0.825147347740668}\n",
      "Epoch 0/10 | loss: 0.8749{'loss': 0.8749, 'grad_norm': 11.136211395263672, 'learning_rate': 1.9025347506132463e-05, 'epoch': 0.8644400785854617}\n",
      "Epoch 0/10 | loss: 0.8751{'loss': 0.8751, 'grad_norm': 26.223432540893555, 'learning_rate': 1.894358135731807e-05, 'epoch': 0.9037328094302554}\n",
      "Epoch 0/10 | loss: 0.9111{'loss': 0.9111, 'grad_norm': 11.878924369812012, 'learning_rate': 1.8861815208503683e-05, 'epoch': 0.9430255402750491}\n",
      "Epoch 0/10 | loss: 0.8624{'loss': 0.8624, 'grad_norm': 6.910865783691406, 'learning_rate': 1.878004905968929e-05, 'epoch': 0.9823182711198428}\n",
      "\n",
      "Epoch 1/10 | eval_loss: 0.8613{'eval_loss': 0.8613367080688477, 'eval_runtime': 7.1138, 'eval_samples_per_second': 1009.864, 'eval_steps_per_second': 31.629, 'epoch': 1.0}\n",
      "Epoch 1/10 | loss: 0.9149{'loss': 0.9149, 'grad_norm': 20.34764289855957, 'learning_rate': 1.86982829108749e-05, 'epoch': 1.0212180746561885}\n",
      "Epoch 1/10 | loss: 0.7849{'loss': 0.7849, 'grad_norm': 6.765448093414307, 'learning_rate': 1.8616516762060507e-05, 'epoch': 1.0605108055009824}\n",
      "Epoch 1/10 | loss: 0.7377{'loss': 0.7377, 'grad_norm': 15.859859466552734, 'learning_rate': 1.853475061324612e-05, 'epoch': 1.099803536345776}\n",
      "Epoch 1/10 | loss: 0.8148{'loss': 0.8148, 'grad_norm': 6.819699287414551, 'learning_rate': 1.8452984464431726e-05, 'epoch': 1.1390962671905698}\n",
      "Epoch 1/10 | loss: 0.8610{'loss': 0.861, 'grad_norm': 12.815388679504395, 'learning_rate': 1.8371218315617335e-05, 'epoch': 1.1783889980353635}\n",
      "Epoch 1/10 | loss: 0.7382{'loss': 0.7382, 'grad_norm': 18.469079971313477, 'learning_rate': 1.8289452166802946e-05, 'epoch': 1.2176817288801571}\n",
      "Epoch 1/10 | loss: 0.8162{'loss': 0.8162, 'grad_norm': 21.21677589416504, 'learning_rate': 1.8207686017988554e-05, 'epoch': 1.2569744597249508}\n",
      "Epoch 1/10 | loss: 0.8041{'loss': 0.8041, 'grad_norm': 13.10856819152832, 'learning_rate': 1.8125919869174166e-05, 'epoch': 1.2962671905697447}\n",
      "Epoch 1/10 | loss: 0.7740{'loss': 0.774, 'grad_norm': 22.579368591308594, 'learning_rate': 1.804415372035977e-05, 'epoch': 1.3355599214145384}\n",
      "Epoch 1/10 | loss: 0.7625{'loss': 0.7625, 'grad_norm': 15.847009658813477, 'learning_rate': 1.7962387571545382e-05, 'epoch': 1.374852652259332}\n",
      "Epoch 1/10 | loss: 0.7801{'loss': 0.7801, 'grad_norm': 12.061300277709961, 'learning_rate': 1.788062142273099e-05, 'epoch': 1.4141453831041257}\n",
      "Epoch 1/10 | loss: 0.7936{'loss': 0.7936, 'grad_norm': 7.02108097076416, 'learning_rate': 1.77988552739166e-05, 'epoch': 1.4534381139489194}\n",
      "Epoch 1/10 | loss: 0.8140{'loss': 0.814, 'grad_norm': 9.175250053405762, 'learning_rate': 1.771708912510221e-05, 'epoch': 1.492730844793713}\n",
      "Epoch 1/10 | loss: 0.7961{'loss': 0.7961, 'grad_norm': 11.774253845214844, 'learning_rate': 1.7635322976287817e-05, 'epoch': 1.5320235756385068}\n",
      "Epoch 1/10 | loss: 0.8054{'loss': 0.8054, 'grad_norm': 19.338964462280273, 'learning_rate': 1.755355682747343e-05, 'epoch': 1.5713163064833005}\n",
      "Epoch 1/10 | loss: 0.7791{'loss': 0.7791, 'grad_norm': 18.16355323791504, 'learning_rate': 1.7471790678659037e-05, 'epoch': 1.6106090373280944}\n",
      "Epoch 1/10 | loss: 0.7669{'loss': 0.7669, 'grad_norm': 5.661188125610352, 'learning_rate': 1.739002452984465e-05, 'epoch': 1.649901768172888}\n",
      "Epoch 1/10 | loss: 0.6934{'loss': 0.6934, 'grad_norm': 12.15493106842041, 'learning_rate': 1.7308258381030253e-05, 'epoch': 1.6891944990176817}\n",
      "Epoch 1/10 | loss: 0.7729{'loss': 0.7729, 'grad_norm': 20.453554153442383, 'learning_rate': 1.7226492232215865e-05, 'epoch': 1.7284872298624756}\n",
      "Epoch 1/10 | loss: 0.8711{'loss': 0.8711, 'grad_norm': 5.895564556121826, 'learning_rate': 1.7144726083401473e-05, 'epoch': 1.7677799607072693}\n",
      "Epoch 1/10 | loss: 0.7997{'loss': 0.7997, 'grad_norm': 10.417804718017578, 'learning_rate': 1.706295993458708e-05, 'epoch': 1.807072691552063}\n",
      "Epoch 1/10 | loss: 0.7930{'loss': 0.793, 'grad_norm': 9.04738998413086, 'learning_rate': 1.6981193785772692e-05, 'epoch': 1.8463654223968566}\n",
      "Epoch 1/10 | loss: 0.7414{'loss': 0.7414, 'grad_norm': 10.176928520202637, 'learning_rate': 1.68994276369583e-05, 'epoch': 1.8856581532416503}\n",
      "Epoch 1/10 | loss: 0.7300{'loss': 0.73, 'grad_norm': 10.296577453613281, 'learning_rate': 1.6817661488143912e-05, 'epoch': 1.924950884086444}\n",
      "Epoch 1/10 | loss: 0.8424{'loss': 0.8424, 'grad_norm': 9.358054161071777, 'learning_rate': 1.673589533932952e-05, 'epoch': 1.9642436149312377}\n",
      "\n",
      "Epoch 2/10 | eval_loss: 0.8474{'eval_loss': 0.8474050164222717, 'eval_runtime': 6.476, 'eval_samples_per_second': 1109.322, 'eval_steps_per_second': 34.744, 'epoch': 2.0}\n",
      "Epoch 2/10 | loss: 0.7695{'loss': 0.7695, 'grad_norm': 7.876089572906494, 'learning_rate': 1.6654129190515128e-05, 'epoch': 2.0031434184675834}\n",
      "Epoch 2/10 | loss: 0.5988{'loss': 0.5988, 'grad_norm': 5.326018333435059, 'learning_rate': 1.6572363041700736e-05, 'epoch': 2.042436149312377}\n",
      "Epoch 2/10 | loss: 0.6310{'loss': 0.631, 'grad_norm': 17.623598098754883, 'learning_rate': 1.6490596892886347e-05, 'epoch': 2.0817288801571707}\n",
      "Epoch 2/10 | loss: 0.6267{'loss': 0.6267, 'grad_norm': 16.58887481689453, 'learning_rate': 1.6408830744071956e-05, 'epoch': 2.121021611001965}\n",
      "Epoch 2/10 | loss: 0.6005{'loss': 0.6005, 'grad_norm': 9.64986801147461, 'learning_rate': 1.6327064595257564e-05, 'epoch': 2.1603143418467585}\n",
      "Epoch 2/10 | loss: 0.7490{'loss': 0.749, 'grad_norm': 6.771092891693115, 'learning_rate': 1.6245298446443175e-05, 'epoch': 2.199607072691552}\n",
      "Epoch 2/10 | loss: 0.6347{'loss': 0.6347, 'grad_norm': 9.853864669799805, 'learning_rate': 1.616516762060507e-05, 'epoch': 2.238899803536346}\n",
      "Epoch 2/10 | loss: 0.5845{'loss': 0.5845, 'grad_norm': 10.368452072143555, 'learning_rate': 1.608340147179068e-05, 'epoch': 2.2781925343811396}\n",
      "Epoch 2/10 | loss: 0.6859{'loss': 0.6859, 'grad_norm': 6.604300498962402, 'learning_rate': 1.600163532297629e-05, 'epoch': 2.3174852652259332}\n",
      "Epoch 2/10 | loss: 0.6414{'loss': 0.6414, 'grad_norm': 9.666332244873047, 'learning_rate': 1.59198691741619e-05, 'epoch': 2.356777996070727}\n",
      "Epoch 2/10 | loss: 0.6598{'loss': 0.6598, 'grad_norm': 75.52740478515625, 'learning_rate': 1.5838103025347507e-05, 'epoch': 2.3960707269155206}\n",
      "Epoch 2/10 | loss: 0.6269{'loss': 0.6269, 'grad_norm': 8.716967582702637, 'learning_rate': 1.5756336876533115e-05, 'epoch': 2.4353634577603143}\n",
      "Epoch 2/10 | loss: 0.6385{'loss': 0.6385, 'grad_norm': 26.40726089477539, 'learning_rate': 1.5674570727718726e-05, 'epoch': 2.474656188605108}\n",
      "Epoch 2/10 | loss: 0.6219{'loss': 0.6219, 'grad_norm': 7.755688667297363, 'learning_rate': 1.5592804578904334e-05, 'epoch': 2.5139489194499016}\n",
      "Epoch 2/10 | loss: 0.6788{'loss': 0.6788, 'grad_norm': 34.17827224731445, 'learning_rate': 1.5511038430089946e-05, 'epoch': 2.5532416502946953}\n",
      "Epoch 2/10 | loss: 0.6325{'loss': 0.6325, 'grad_norm': 5.988651275634766, 'learning_rate': 1.5429272281275554e-05, 'epoch': 2.5925343811394894}\n",
      "Epoch 2/10 | loss: 0.6243{'loss': 0.6243, 'grad_norm': 8.751041412353516, 'learning_rate': 1.5347506132461162e-05, 'epoch': 2.6318271119842827}\n",
      "Epoch 2/10 | loss: 0.7026{'loss': 0.7026, 'grad_norm': 13.22811508178711, 'learning_rate': 1.5265739983646773e-05, 'epoch': 2.6711198428290768}\n",
      "Epoch 2/10 | loss: 0.7595{'loss': 0.7595, 'grad_norm': 8.219080924987793, 'learning_rate': 1.5183973834832381e-05, 'epoch': 2.7104125736738705}\n",
      "Epoch 2/10 | loss: 0.7530{'loss': 0.753, 'grad_norm': 11.10689926147461, 'learning_rate': 1.510220768601799e-05, 'epoch': 2.749705304518664}\n",
      "Epoch 2/10 | loss: 0.6799{'loss': 0.6799, 'grad_norm': 17.427467346191406, 'learning_rate': 1.5020441537203598e-05, 'epoch': 2.788998035363458}\n",
      "Epoch 2/10 | loss: 0.6600{'loss': 0.66, 'grad_norm': 54.95343017578125, 'learning_rate': 1.4938675388389207e-05, 'epoch': 2.8282907662082515}\n",
      "Epoch 2/10 | loss: 0.6292{'loss': 0.6292, 'grad_norm': 56.854312896728516, 'learning_rate': 1.4856909239574817e-05, 'epoch': 2.867583497053045}\n",
      "Epoch 2/10 | loss: 0.6519{'loss': 0.6519, 'grad_norm': 23.305740356445312, 'learning_rate': 1.4775143090760427e-05, 'epoch': 2.906876227897839}\n",
      "Epoch 2/10 | loss: 0.6726{'loss': 0.6726, 'grad_norm': 22.05587387084961, 'learning_rate': 1.4693376941946037e-05, 'epoch': 2.9461689587426325}\n",
      "Epoch 2/10 | loss: 0.6590{'loss': 0.659, 'grad_norm': 20.271991729736328, 'learning_rate': 1.4611610793131645e-05, 'epoch': 2.985461689587426}\n",
      "\n",
      "Epoch 3/10 | eval_loss: 0.9467{'eval_loss': 0.9467149376869202, 'eval_runtime': 6.5065, 'eval_samples_per_second': 1104.123, 'eval_steps_per_second': 34.581, 'epoch': 3.0}\n",
      "Epoch 3/10 | loss: 0.5821{'loss': 0.5821, 'grad_norm': 8.081766128540039, 'learning_rate': 1.4529844644317254e-05, 'epoch': 3.024361493123772}\n",
      "Epoch 3/10 | loss: 0.5341{'loss': 0.5341, 'grad_norm': 27.44788932800293, 'learning_rate': 1.4448078495502863e-05, 'epoch': 3.063654223968566}\n",
      "Epoch 3/10 | loss: 0.5685{'loss': 0.5685, 'grad_norm': 5.550387382507324, 'learning_rate': 1.436631234668847e-05, 'epoch': 3.1029469548133597}\n",
      "Epoch 3/10 | loss: 0.4904{'loss': 0.4904, 'grad_norm': 8.89039134979248, 'learning_rate': 1.428454619787408e-05, 'epoch': 3.1422396856581534}\n",
      "Epoch 3/10 | loss: 0.6068{'loss': 0.6068, 'grad_norm': 9.151265144348145, 'learning_rate': 1.420278004905969e-05, 'epoch': 3.181532416502947}\n",
      "Epoch 3/10 | loss: 0.5785{'loss': 0.5785, 'grad_norm': 32.964046478271484, 'learning_rate': 1.41210139002453e-05, 'epoch': 3.2208251473477407}\n",
      "Epoch 3/10 | loss: 0.5560{'loss': 0.556, 'grad_norm': 4.481537342071533, 'learning_rate': 1.403924775143091e-05, 'epoch': 3.2601178781925344}\n",
      "Epoch 3/10 | loss: 0.6052{'loss': 0.6052, 'grad_norm': 11.229964256286621, 'learning_rate': 1.3957481602616518e-05, 'epoch': 3.299410609037328}\n",
      "Epoch 3/10 | loss: 0.6341{'loss': 0.6341, 'grad_norm': 11.094198226928711, 'learning_rate': 1.3875715453802128e-05, 'epoch': 3.3387033398821218}\n",
      "Epoch 3/10 | loss: 0.5280{'loss': 0.528, 'grad_norm': 32.51204299926758, 'learning_rate': 1.3793949304987737e-05, 'epoch': 3.3779960707269154}\n",
      "Epoch 3/10 | loss: 0.5578{'loss': 0.5578, 'grad_norm': 28.566688537597656, 'learning_rate': 1.3712183156173344e-05, 'epoch': 3.417288801571709}\n",
      "Epoch 3/10 | loss: 0.4992{'loss': 0.4992, 'grad_norm': 7.133031845092773, 'learning_rate': 1.3630417007358953e-05, 'epoch': 3.456581532416503}\n",
      "Epoch 3/10 | loss: 0.5532{'loss': 0.5532, 'grad_norm': 56.86167907714844, 'learning_rate': 1.3548650858544563e-05, 'epoch': 3.4958742632612965}\n",
      "Epoch 3/10 | loss: 0.4917{'loss': 0.4917, 'grad_norm': 49.45718002319336, 'learning_rate': 1.3466884709730173e-05, 'epoch': 3.53516699410609}\n",
      "Epoch 3/10 | loss: 0.4929{'loss': 0.4929, 'grad_norm': 5.79961633682251, 'learning_rate': 1.3385118560915781e-05, 'epoch': 3.5744597249508843}\n",
      "Epoch 3/10 | loss: 0.5522{'loss': 0.5522, 'grad_norm': 13.241495132446289, 'learning_rate': 1.330335241210139e-05, 'epoch': 3.613752455795678}\n",
      "Epoch 3/10 | loss: 0.5150{'loss': 0.515, 'grad_norm': 24.838394165039062, 'learning_rate': 1.3221586263287e-05, 'epoch': 3.6530451866404716}\n",
      "Epoch 3/10 | loss: 0.5549{'loss': 0.5549, 'grad_norm': 39.043399810791016, 'learning_rate': 1.3141455437448898e-05, 'epoch': 3.6923379174852653}\n",
      "Epoch 3/10 | loss: 0.5505{'loss': 0.5505, 'grad_norm': 28.31899070739746, 'learning_rate': 1.3059689288634508e-05, 'epoch': 3.731630648330059}\n",
      "Epoch 3/10 | loss: 0.5236{'loss': 0.5236, 'grad_norm': 13.457663536071777, 'learning_rate': 1.2977923139820114e-05, 'epoch': 3.7709233791748527}\n",
      "Epoch 3/10 | loss: 0.5334{'loss': 0.5334, 'grad_norm': 42.38497543334961, 'learning_rate': 1.2896156991005724e-05, 'epoch': 3.8102161100196463}\n",
      "Epoch 3/10 | loss: 0.5827{'loss': 0.5827, 'grad_norm': 17.556346893310547, 'learning_rate': 1.2814390842191334e-05, 'epoch': 3.84950884086444}\n",
      "Epoch 3/10 | loss: 0.5748{'loss': 0.5748, 'grad_norm': 14.177979469299316, 'learning_rate': 1.2732624693376942e-05, 'epoch': 3.8888015717092337}\n",
      "Epoch 3/10 | loss: 0.6211{'loss': 0.6211, 'grad_norm': 67.9521713256836, 'learning_rate': 1.2650858544562552e-05, 'epoch': 3.9280943025540274}\n",
      "Epoch 3/10 | loss: 0.4992{'loss': 0.4992, 'grad_norm': 9.848785400390625, 'learning_rate': 1.2569092395748161e-05, 'epoch': 3.967387033398821}\n",
      "\n",
      "Epoch 4/10 | eval_loss: 1.0170{'eval_loss': 1.017003059387207, 'eval_runtime': 6.4687, 'eval_samples_per_second': 1110.581, 'eval_steps_per_second': 34.783, 'epoch': 4.0}\n",
      "Epoch 4/10 | loss: 0.4962{'loss': 0.4962, 'grad_norm': 14.307477951049805, 'learning_rate': 1.2487326246933771e-05, 'epoch': 4.006286836935167}\n",
      "Epoch 4/10 | loss: 0.4441{'loss': 0.4441, 'grad_norm': 7.810914993286133, 'learning_rate': 1.2405560098119381e-05, 'epoch': 4.045579567779961}\n",
      "Epoch 4/10 | loss: 0.4632{'loss': 0.4632, 'grad_norm': 9.282155990600586, 'learning_rate': 1.2323793949304989e-05, 'epoch': 4.084872298624754}\n",
      "Epoch 4/10 | loss: 0.4019{'loss': 0.4019, 'grad_norm': 7.689640045166016, 'learning_rate': 1.2242027800490597e-05, 'epoch': 4.124165029469548}\n",
      "Epoch 4/10 | loss: 0.4155{'loss': 0.4155, 'grad_norm': 26.548189163208008, 'learning_rate': 1.2160261651676207e-05, 'epoch': 4.1634577603143414}\n",
      "Epoch 4/10 | loss: 0.4165{'loss': 0.4165, 'grad_norm': 9.386773109436035, 'learning_rate': 1.2078495502861815e-05, 'epoch': 4.202750491159136}\n",
      "Epoch 4/10 | loss: 0.5035{'loss': 0.5035, 'grad_norm': 6.568237781524658, 'learning_rate': 1.1996729354047425e-05, 'epoch': 4.24204322200393}\n",
      "Epoch 4/10 | loss: 0.4232{'loss': 0.4232, 'grad_norm': 21.246355056762695, 'learning_rate': 1.1914963205233035e-05, 'epoch': 4.281335952848723}\n",
      "Epoch 4/10 | loss: 0.3806{'loss': 0.3806, 'grad_norm': 9.555648803710938, 'learning_rate': 1.1833197056418644e-05, 'epoch': 4.320628683693517}\n",
      "Epoch 4/10 | loss: 0.4455{'loss': 0.4455, 'grad_norm': 6.4449286460876465, 'learning_rate': 1.1751430907604252e-05, 'epoch': 4.35992141453831}\n",
      "Epoch 4/10 | loss: 0.4604{'loss': 0.4604, 'grad_norm': 13.874938011169434, 'learning_rate': 1.1669664758789862e-05, 'epoch': 4.399214145383104}\n",
      "Epoch 4/10 | loss: 0.4291{'loss': 0.4291, 'grad_norm': 14.803590774536133, 'learning_rate': 1.1587898609975472e-05, 'epoch': 4.438506876227898}\n",
      "Epoch 4/10 | loss: 0.4481{'loss': 0.4481, 'grad_norm': 58.465206146240234, 'learning_rate': 1.150613246116108e-05, 'epoch': 4.477799607072692}\n",
      "Epoch 4/10 | loss: 0.4626{'loss': 0.4626, 'grad_norm': 20.678693771362305, 'learning_rate': 1.1424366312346688e-05, 'epoch': 4.517092337917485}\n",
      "Epoch 4/10 | loss: 0.4877{'loss': 0.4877, 'grad_norm': 8.712647438049316, 'learning_rate': 1.1342600163532298e-05, 'epoch': 4.556385068762279}\n",
      "Epoch 4/10 | loss: 0.4666{'loss': 0.4666, 'grad_norm': 34.70836639404297, 'learning_rate': 1.1260834014717908e-05, 'epoch': 4.595677799607072}\n",
      "Epoch 4/10 | loss: 0.3964{'loss': 0.3964, 'grad_norm': 6.6298017501831055, 'learning_rate': 1.1179067865903517e-05, 'epoch': 4.6349705304518665}\n",
      "Epoch 4/10 | loss: 0.4974{'loss': 0.4974, 'grad_norm': 9.74730396270752, 'learning_rate': 1.1097301717089125e-05, 'epoch': 4.674263261296661}\n",
      "Epoch 4/10 | loss: 0.4351{'loss': 0.4351, 'grad_norm': 7.88358211517334, 'learning_rate': 1.1017170891251023e-05, 'epoch': 4.713555992141454}\n",
      "Epoch 4/10 | loss: 0.4241{'loss': 0.4241, 'grad_norm': 47.6654052734375, 'learning_rate': 1.0935404742436633e-05, 'epoch': 4.752848722986247}\n",
      "Epoch 4/10 | loss: 0.4681{'loss': 0.4681, 'grad_norm': 116.61553955078125, 'learning_rate': 1.0853638593622243e-05, 'epoch': 4.792141453831041}\n",
      "Epoch 4/10 | loss: 0.4694{'loss': 0.4694, 'grad_norm': 8.088982582092285, 'learning_rate': 1.0771872444807849e-05, 'epoch': 4.831434184675835}\n",
      "Epoch 4/10 | loss: 0.4363{'loss': 0.4363, 'grad_norm': 12.578654289245605, 'learning_rate': 1.0690106295993459e-05, 'epoch': 4.8707269155206285}\n",
      "Epoch 4/10 | loss: 0.4335{'loss': 0.4335, 'grad_norm': 24.155038833618164, 'learning_rate': 1.0608340147179068e-05, 'epoch': 4.910019646365423}\n",
      "Epoch 4/10 | loss: 0.5476{'loss': 0.5476, 'grad_norm': 43.36695861816406, 'learning_rate': 1.0526573998364678e-05, 'epoch': 4.949312377210216}\n",
      "Epoch 4/10 | loss: 0.4260{'loss': 0.426, 'grad_norm': 81.216552734375, 'learning_rate': 1.0444807849550286e-05, 'epoch': 4.98860510805501}\n",
      "\n",
      "Epoch 5/10 | eval_loss: 1.1470{'eval_loss': 1.1470142602920532, 'eval_runtime': 6.5032, 'eval_samples_per_second': 1104.684, 'eval_steps_per_second': 34.598, 'epoch': 5.0}\n",
      "Epoch 5/10 | loss: 0.4346{'loss': 0.4346, 'grad_norm': 25.15949249267578, 'learning_rate': 1.0363041700735896e-05, 'epoch': 5.027504911591356}\n",
      "Epoch 5/10 | loss: 0.3962{'loss': 0.3962, 'grad_norm': 12.611687660217285, 'learning_rate': 1.0281275551921506e-05, 'epoch': 5.066797642436149}\n",
      "Epoch 5/10 | loss: 0.3571{'loss': 0.3571, 'grad_norm': 25.29111671447754, 'learning_rate': 1.0199509403107116e-05, 'epoch': 5.106090373280943}\n",
      "Epoch 5/10 | loss: 0.3553{'loss': 0.3553, 'grad_norm': 5.402877330780029, 'learning_rate': 1.0117743254292724e-05, 'epoch': 5.145383104125736}\n",
      "Epoch 5/10 | loss: 0.3693{'loss': 0.3693, 'grad_norm': 3.7851216793060303, 'learning_rate': 1.0035977105478332e-05, 'epoch': 5.18467583497053}\n",
      "Epoch 5/10 | loss: 0.2848{'loss': 0.2848, 'grad_norm': 23.53738021850586, 'learning_rate': 9.954210956663942e-06, 'epoch': 5.2239685658153245}\n",
      "Epoch 5/10 | loss: 0.3453{'loss': 0.3453, 'grad_norm': 8.958696365356445, 'learning_rate': 9.872444807849551e-06, 'epoch': 5.263261296660118}\n",
      "Epoch 5/10 | loss: 0.3591{'loss': 0.3591, 'grad_norm': 10.832437515258789, 'learning_rate': 9.79067865903516e-06, 'epoch': 5.302554027504912}\n",
      "Epoch 5/10 | loss: 0.3557{'loss': 0.3557, 'grad_norm': 36.051639556884766, 'learning_rate': 9.70891251022077e-06, 'epoch': 5.341846758349705}\n",
      "Epoch 5/10 | loss: 0.3438{'loss': 0.3438, 'grad_norm': 62.63501739501953, 'learning_rate': 9.627146361406379e-06, 'epoch': 5.381139489194499}\n",
      "Epoch 5/10 | loss: 0.3328{'loss': 0.3328, 'grad_norm': 18.138708114624023, 'learning_rate': 9.545380212591989e-06, 'epoch': 5.4204322200392925}\n",
      "Epoch 5/10 | loss: 0.3528{'loss': 0.3528, 'grad_norm': 12.908345222473145, 'learning_rate': 9.463614063777597e-06, 'epoch': 5.459724950884087}\n",
      "Epoch 5/10 | loss: 0.3792{'loss': 0.3792, 'grad_norm': 9.381166458129883, 'learning_rate': 9.381847914963207e-06, 'epoch': 5.49901768172888}\n",
      "Epoch 5/10 | loss: 0.3966{'loss': 0.3966, 'grad_norm': 4.971973896026611, 'learning_rate': 9.300081766148815e-06, 'epoch': 5.538310412573674}\n",
      "Epoch 5/10 | loss: 0.3780{'loss': 0.378, 'grad_norm': 12.174419403076172, 'learning_rate': 9.218315617334424e-06, 'epoch': 5.577603143418468}\n",
      "Epoch 5/10 | loss: 0.3635{'loss': 0.3635, 'grad_norm': 19.514759063720703, 'learning_rate': 9.136549468520034e-06, 'epoch': 5.616895874263261}\n",
      "Epoch 5/10 | loss: 0.3824{'loss': 0.3824, 'grad_norm': 8.891984939575195, 'learning_rate': 9.054783319705642e-06, 'epoch': 5.6561886051080545}\n",
      "Epoch 5/10 | loss: 0.3646{'loss': 0.3646, 'grad_norm': 6.555848121643066, 'learning_rate': 8.973017170891252e-06, 'epoch': 5.695481335952849}\n",
      "Epoch 5/10 | loss: 0.3796{'loss': 0.3796, 'grad_norm': 13.815539360046387, 'learning_rate': 8.891251022076862e-06, 'epoch': 5.734774066797643}\n",
      "Epoch 5/10 | loss: 0.4075{'loss': 0.4075, 'grad_norm': 4.942131042480469, 'learning_rate': 8.80948487326247e-06, 'epoch': 5.774066797642436}\n",
      "Epoch 5/10 | loss: 0.3095{'loss': 0.3095, 'grad_norm': 11.8516206741333, 'learning_rate': 8.727718724448078e-06, 'epoch': 5.81335952848723}\n",
      "Epoch 5/10 | loss: 0.3740{'loss': 0.374, 'grad_norm': 10.099817276000977, 'learning_rate': 8.645952575633688e-06, 'epoch': 5.852652259332023}\n",
      "Epoch 5/10 | loss: 0.3635{'loss': 0.3635, 'grad_norm': 7.06480073928833, 'learning_rate': 8.564186426819297e-06, 'epoch': 5.8919449901768175}\n",
      "Epoch 5/10 | loss: 0.4327{'loss': 0.4327, 'grad_norm': 21.600194931030273, 'learning_rate': 8.482420278004907e-06, 'epoch': 5.931237721021611}\n",
      "Epoch 5/10 | loss: 0.3283{'loss': 0.3283, 'grad_norm': 11.2774658203125, 'learning_rate': 8.400654129190515e-06, 'epoch': 5.970530451866405}\n",
      "\n",
      "Epoch 6/10 | eval_loss: 1.2210{'eval_loss': 1.2209527492523193, 'eval_runtime': 6.5526, 'eval_samples_per_second': 1096.365, 'eval_steps_per_second': 34.338, 'epoch': 6.0}\n",
      "Epoch 6/10 | loss: 0.3269{'loss': 0.3269, 'grad_norm': 11.824267387390137, 'learning_rate': 8.318887980376125e-06, 'epoch': 6.0094302554027506}\n",
      "Epoch 6/10 | loss: 0.3054{'loss': 0.3054, 'grad_norm': 11.737016677856445, 'learning_rate': 8.237121831561735e-06, 'epoch': 6.048722986247544}\n",
      "Epoch 6/10 | loss: 0.2756{'loss': 0.2756, 'grad_norm': 47.95787811279297, 'learning_rate': 8.155355682747343e-06, 'epoch': 6.088015717092338}\n",
      "Epoch 6/10 | loss: 0.2963{'loss': 0.2963, 'grad_norm': 24.963424682617188, 'learning_rate': 8.073589533932953e-06, 'epoch': 6.127308447937132}\n",
      "Epoch 6/10 | loss: 0.3026{'loss': 0.3026, 'grad_norm': 154.14163208007812, 'learning_rate': 7.99182338511856e-06, 'epoch': 6.166601178781925}\n",
      "Epoch 6/10 | loss: 0.3213{'loss': 0.3213, 'grad_norm': 8.320034980773926, 'learning_rate': 7.91005723630417e-06, 'epoch': 6.205893909626719}\n",
      "Epoch 6/10 | loss: 0.3010{'loss': 0.301, 'grad_norm': 4.1325531005859375, 'learning_rate': 7.82829108748978e-06, 'epoch': 6.245186640471513}\n",
      "Epoch 6/10 | loss: 0.2896{'loss': 0.2896, 'grad_norm': 7.521757125854492, 'learning_rate': 7.74652493867539e-06, 'epoch': 6.284479371316307}\n",
      "Epoch 6/10 | loss: 0.2577{'loss': 0.2577, 'grad_norm': 10.687477111816406, 'learning_rate': 7.664758789860998e-06, 'epoch': 6.3237721021611}\n",
      "Epoch 6/10 | loss: 0.2936{'loss': 0.2936, 'grad_norm': 13.84988784790039, 'learning_rate': 7.582992641046607e-06, 'epoch': 6.363064833005894}\n",
      "Epoch 6/10 | loss: 0.3216{'loss': 0.3216, 'grad_norm': 5.87170934677124, 'learning_rate': 7.501226492232216e-06, 'epoch': 6.402357563850687}\n",
      "Epoch 6/10 | loss: 0.3337{'loss': 0.3337, 'grad_norm': 9.107915878295898, 'learning_rate': 7.419460343417826e-06, 'epoch': 6.4416502946954814}\n",
      "Epoch 6/10 | loss: 0.3140{'loss': 0.314, 'grad_norm': 10.071269989013672, 'learning_rate': 7.3376941946034356e-06, 'epoch': 6.480943025540275}\n",
      "Epoch 6/10 | loss: 0.3359{'loss': 0.3359, 'grad_norm': 6.172966957092285, 'learning_rate': 7.255928045789044e-06, 'epoch': 6.520235756385069}\n",
      "Epoch 6/10 | loss: 0.3089{'loss': 0.3089, 'grad_norm': 17.452558517456055, 'learning_rate': 7.1741618969746526e-06, 'epoch': 6.559528487229862}\n",
      "Epoch 6/10 | loss: 0.3403{'loss': 0.3403, 'grad_norm': 7.556502342224121, 'learning_rate': 7.092395748160262e-06, 'epoch': 6.598821218074656}\n",
      "Epoch 6/10 | loss: 0.3088{'loss': 0.3088, 'grad_norm': 5.048868179321289, 'learning_rate': 7.010629599345872e-06, 'epoch': 6.63811394891945}\n",
      "Epoch 6/10 | loss: 0.3007{'loss': 0.3007, 'grad_norm': 6.847073554992676, 'learning_rate': 6.92886345053148e-06, 'epoch': 6.6774066797642435}\n",
      "Epoch 6/10 | loss: 0.3397{'loss': 0.3397, 'grad_norm': 8.279074668884277, 'learning_rate': 6.847097301717089e-06, 'epoch': 6.716699410609038}\n",
      "Epoch 6/10 | loss: 0.2866{'loss': 0.2866, 'grad_norm': 16.613445281982422, 'learning_rate': 6.765331152902699e-06, 'epoch': 6.755992141453831}\n",
      "Epoch 6/10 | loss: 0.3273{'loss': 0.3273, 'grad_norm': 8.378373146057129, 'learning_rate': 6.683565004088309e-06, 'epoch': 6.795284872298625}\n",
      "Epoch 6/10 | loss: 0.3125{'loss': 0.3125, 'grad_norm': 18.67226791381836, 'learning_rate': 6.6017988552739176e-06, 'epoch': 6.834577603143418}\n",
      "Epoch 6/10 | loss: 0.2521{'loss': 0.2521, 'grad_norm': 3.4242124557495117, 'learning_rate': 6.520032706459526e-06, 'epoch': 6.873870333988212}\n",
      "Epoch 6/10 | loss: 0.3311{'loss': 0.3311, 'grad_norm': 11.64187240600586, 'learning_rate': 6.438266557645135e-06, 'epoch': 6.913163064833006}\n",
      "Epoch 6/10 | loss: 0.2666{'loss': 0.2666, 'grad_norm': 10.271214485168457, 'learning_rate': 6.356500408830744e-06, 'epoch': 6.9524557956778}\n",
      "Epoch 6/10 | loss: 0.2598{'loss': 0.2598, 'grad_norm': 9.519498825073242, 'learning_rate': 6.274734260016354e-06, 'epoch': 6.991748526522593}\n",
      "\n",
      "Epoch 7/10 | eval_loss: 1.4404{'eval_loss': 1.4404222965240479, 'eval_runtime': 6.3537, 'eval_samples_per_second': 1130.688, 'eval_steps_per_second': 35.413, 'epoch': 7.0}\n",
      "Epoch 7/10{'train_runtime': 1128.4752, 'train_samples_per_second': 360.717, 'train_steps_per_second': 11.281, 'train_loss': 0.5891541110828473, 'epoch': 7.0}\n",
      "\n",
      "Training completed\n",
      "\n",
      "\n",
      "==================================================\n",
      "Training Model 2/3: Word2Vec + SVM\n",
      "==================================================\n",
      "\n",
      "Training Word2Vec + SVM...\n",
      "   Loading Word2Vec model... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training SVM... Done\n",
      "\n",
      "==================================================\n",
      "Training Model 3/3: TF-IDF + Naive Bayes\n",
      "==================================================\n",
      "\n",
      "Training TF-IDF + Naive Bayes...\n",
      "   Vectorizing... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training NB..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done\n",
      "\n",
      "Training Meta-Learner...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 21.8 minutes\n",
      "\n",
      "Predicting on Test Set...\n",
      "   Generating predictions from base models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Applying meta-learner...\n",
      "\n",
      "Predictions generated for 16281 samples\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3.5 Main Pipeline: Train and Predict ====================\n",
    "\n",
    "# Prepare data\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['emotion'].tolist()\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_ids = test_df['id'].tolist()\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# Train stacking ensemble (all 3 base models + meta-learner)\n",
    "models, meta_learner, label_encoder = train_stacking_ensemble(train_texts, train_labels)\n",
    "\n",
    "# Generate predictions on test set\n",
    "predictions = predict_stacking_ensemble(models, meta_learner, test_texts)\n",
    "\n",
    "# Decode numeric predictions back to emotion labels\n",
    "predicted_emotions = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "print(f\"\\nPredictions generated for {len(predicted_emotions)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 3.6 Generate Submission File ====================\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'emotion': predicted_emotions\n",
    "})\n",
    "\n",
    "# Generate timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_file = f\"results/submission_{timestamp}.csv\"\n",
    "\n",
    "# Ensure results directory exists\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save submission CSV\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_file}\")\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "print(submission_df['emotion'].value_counts())\n",
    "\n",
    "# Preview first 10 predictions\n",
    "print(f\"\\nSample Predictions:\")\n",
    "display(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
